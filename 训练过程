训练过程中最神奇的是，必须先采用1e-3的学习率训练50轮，待到模型参数较佳时，再解开所有冰冻层，将学习率改为1e-4，才能取得这种最佳权重效果。

如果最开始就解冻所有层，将训练得到NAN。
如果不解冻，仅仅使用1e-3不断训练， val loss达到瓶颈时的效果很差，此后也无法再继续下降。

只有按上面这种训练方式，val loss才能降低，达到最佳训练效果。

而且不得不承认，用这种方式训练出来的模型，检测效果比之前训练出来的好的多，复杂场景的检验精度也足够好。


# Epoch 1/50
# 250/250 [==============================] - 2937s 12s/step - loss: 765.7747 - val_loss: 52.5297
# Epoch 2/50
# 250/250 [==============================] - 2832s 11s/step - loss: 47.7648 - val_loss: 39.5398
# Epoch 3/50
# 250/250 [==============================] - 2807s 11s/step - loss: 38.4640 - val_loss: 35.3263
# Epoch 4/50
# 250/250 [==============================] - 2796s 11s/step - loss: 35.5546 - val_loss: 33.4273
# Epoch 5/50
# 250/250 [==============================] - 2828s 11s/step - loss: 33.0305 - val_loss: 31.7820
# Epoch 6/50
# 250/250 [==============================] - 2806s 11s/step - loss: 32.2785 - val_loss: 30.5448
# Epoch 7/50
# 250/250 [==============================] - 2802s 11s/step - loss: 31.3935 - val_loss: 30.1925
# Epoch 8/50
# 250/250 [==============================] - 2800s 11s/step - loss: 30.0459 - val_loss: 29.1169
# Epoch 9/50
# 250/250 [==============================] - 2781s 11s/step - loss: 30.0779 - val_loss: 28.6489
# Epoch 10/50
# 250/250 [==============================] - 2820s 11s/step - loss: 28.4846 - val_loss: 28.6680
# Epoch 11/50
# 250/250 [==============================] - 2799s 11s/step - loss: 28.2774 - val_loss: 27.7473
# Epoch 12/50
# 250/250 [==============================] - 2812s 11s/step - loss: 27.6891 - val_loss: 27.5067
# Epoch 13/50
# 250/250 [==============================] - 2789s 11s/step - loss: 27.4151 - val_loss: 26.7852
# Epoch 14/50
# 250/250 [==============================] - 2783s 11s/step - loss: 27.0420 - val_loss: 26.6191
# Epoch 15/50
# 250/250 [==============================] - 3331s 13s/step - loss: 26.7571 - val_loss: 27.0306
# Epoch 16/50
# 250/250 [==============================] - 3461s 14s/step - loss: 26.0941 - val_loss: 26.4233
# Epoch 17/50
# 250/250 [==============================] - 3610s 14s/step - loss: 25.9667 - val_loss: 25.1764
# Epoch 18/50
# 250/250 [==============================] - 3540s 14s/step - loss: 25.7184 - val_loss: 25.1063
# Epoch 19/50
# 250/250 [==============================] - 3499s 14s/step - loss: 25.7022 - val_loss: 26.3312
# Epoch 20/50
# 250/250 [==============================] - 3573s 14s/step - loss: 25.5391 - val_loss: 24.9430
# Epoch 21/50
# 250/250 [==============================] - 3500s 14s/step - loss: 25.4440 - val_loss: 25.0265
# Epoch 22/50
# 250/250 [==============================] - 3584s 14s/step - loss: 25.1920 - val_loss: 24.6450
# Epoch 23/50
# 250/250 [==============================] - 3474s 14s/step - loss: 25.1607 - val_loss: 25.1206
# Epoch 24/50
# 250/250 [==============================] - 3466s 14s/step - loss: 24.9211 - val_loss: 24.4269
# Epoch 25/50
# 250/250 [==============================] - 3552s 14s/step - loss: 24.9371 - val_loss: 24.3136
# Epoch 26/50
# 250/250 [==============================] - 3465s 14s/step - loss: 24.8315 - val_loss: 24.2697
# Epoch 27/50
# 250/250 [==============================] - 3655s 15s/step - loss: 24.5633 - val_loss: 24.4851
# Epoch 28/50
# 250/250 [==============================] - 3505s 14s/step - loss: 24.7429 - val_loss: 24.1806
# Epoch 29/50
# 250/250 [==============================] - 3503s 14s/step - loss: 24.4981 - val_loss: 23.5894
# Epoch 30/50
# 250/250 [==============================] - 3620s 14s/step - loss: 24.0732 - val_loss: 25.0998
# Epoch 31/50
# 250/250 [==============================] - 3504s 14s/step - loss: 24.2744 - val_loss: 23.7604
# Epoch 32/50
# 250/250 [==============================] - 3597s 14s/step - loss: 24.2878 - val_loss: 23.8372
#
# Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
# Epoch 33/50
# 250/250 [==============================] - 3127s 13s/step - loss: 23.9576 - val_loss: 23.8266
# Epoch 34/50
# 250/250 [==============================] - 2818s 11s/step - loss: 23.8883 - val_loss: 23.6425
# Epoch 35/50
# 250/250 [==============================] - 2815s 11s/step - loss: 23.7997 - val_loss: 24.1633
#
# Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
# Epoch 36/50
# 250/250 [==============================] - 2781s 11s/step - loss: 23.3998 - val_loss: 23.0662
# Epoch 37/50
# 250/250 [==============================] - 2743s 11s/step - loss: 23.6843 - val_loss: 23.7939
# Epoch 38/50
# 250/250 [==============================] - 2817s 11s/step - loss: 23.6033 - val_loss: 23.5294
# Epoch 39/50
# 250/250 [==============================] - 2796s 11s/step - loss: 23.7276 - val_loss: 23.3892
#
# Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
# Epoch 40/50
# 250/250 [==============================] - 2805s 11s/step - loss: 23.4771 - val_loss: 23.7138
# Epoch 41/50
# 250/250 [==============================] - 2814s 11s/step - loss: 23.5621 - val_loss: 23.5275
# Epoch 42/50
# 250/250 [==============================] - 2780s 11s/step - loss: 23.4889 - val_loss: 23.8720
#
# Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
# Epoch 43/50
# 250/250 [==============================] - 2796s 11s/step - loss: 23.6455 - val_loss: 23.3862
# Epoch 44/50
# 250/250 [==============================] - 2773s 11s/step - loss: 23.1683 - val_loss: 23.0743
# Epoch 45/50
# 250/250 [==============================] - 2773s 11s/step - loss: 23.5555 - val_loss: 23.5851
#
# Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
# Epoch 46/50
# 250/250 [==============================] - 2789s 11s/step - loss: 23.8222 - val_loss: 23.9545
# Epoch 47/50
# 250/250 [==============================] - 2771s 11s/step - loss: 23.7860 - val_loss: 22.9582
# Epoch 48/50
# 250/250 [==============================] - 2801s 11s/step - loss: 23.4268 - val_loss: 24.2679
# Epoch 49/50
# 250/250 [==============================] - 2789s 11s/step - loss: 23.5236 - val_loss: 22.9590
# Epoch 50/50
# 250/250 [==============================] - 2792s 11s/step - loss: 23.1953 - val_loss: 23.6370

# Epoch 51/100
# 250/250 [==============================] - 16564s 66s/step - loss: 23.1127 - val_loss: 22.4485
# Epoch 52/100
# 250/250 [==============================] - 14795s 59s/step - loss: 21.7915 - val_loss: 22.1340
# Epoch 53/100
# 250/250 [==============================] - 14684s 59s/step - loss: 21.4848 - val_loss: 22.1087
# Epoch 54/100
# 250/250 [==============================] - 14682s 59s/step - loss: 21.1646 - val_loss: 22.0548
# Epoch 55/100
# 250/250 [==============================] - 14542s 58s/step - loss: 21.0497 - val_loss: 21.9001
# Epoch 56/100
# 250/250 [==============================] - 14517s 58s/step - loss: 20.8310 - val_loss: 21.6913
# Epoch 57/100
# 250/250 [==============================] - 14604s 58s/step - loss: 20.5699 - val_loss: 21.7369
# Epoch 58/100
# 250/250 [==============================] - 7107s 28s/step - loss: 20.6084 - val_loss: 21.9203
# Epoch 59/100
# 250/250 [==============================] - 7139s 29s/step - loss: 20.5931 - val_loss: 21.6692
# Epoch 60/100
# 250/250 [==============================] - 6956s 28s/step - loss: 20.3675 - val_loss: 21.3965
# Epoch 61/100
# 250/250 [==============================] - 6977s 28s/step - loss: 19.9916 - val_loss: 21.1340
# Epoch 62/100
# 250/250 [==============================] - 6884s 28s/step - loss: 20.1546 - val_loss: 21.7824
# Epoch 63/100
# 250/250 [==============================] - 6856s 27s/step - loss: 19.7350 - val_loss: 21.8019
# Epoch 64/100
# 250/250 [==============================] - 6874s 28s/step - loss: 19.9041 - val_loss: 21.0918
# Epoch 65/100
# 250/250 [==============================] - 6873s 27s/step - loss: 19.5740 - val_loss: 21.4807
# Epoch 66/100
# 250/250 [==============================] - 6835s 27s/step - loss: 19.2460 - val_loss: 21.5660
# Epoch 67/100
# 250/250 [==============================] - 6880s 28s/step - loss: 19.5531 - val_loss: 21.2373
#
# Epoch 00067: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
# Epoch 68/100
# 250/250 [==============================] - 6867s 27s/step - loss: 18.5978 - val_loss: 20.4263
# Epoch 69/100
# 250/250 [==============================] - 6916s 28s/step - loss: 18.2992 - val_loss: 21.3976
# Epoch 70/100
# 250/250 [==============================] - 6865s 27s/step - loss: 18.0120 - val_loss: 20.3708
# Epoch 71/100
# 250/250 [==============================] - 6830s 27s/step - loss: 17.9274 - val_loss: 20.3196
# Epoch 72/100
# 250/250 [==============================] - 6904s 28s/step - loss: 17.9202 - val_loss: 20.3985
# Epoch 73/100
# 250/250 [==============================] - 6871s 27s/step - loss: 17.6777 - val_loss: 20.4611
# Epoch 74/100
# 250/250 [==============================] - 6845s 27s/step - loss: 17.6884 - val_loss: 19.7344
# Epoch 75/100
# 250/250 [==============================] - 6834s 27s/step - loss: 17.5878 - val_loss: 20.1907
# Epoch 76/100
# 250/250 [==============================] - 6869s 27s/step - loss: 17.3705 - val_loss: 20.5968
# Epoch 77/100
# 250/250 [==============================] - 6827s 27s/step - loss: 17.2221 - val_loss: 19.7772
#
# Epoch 00077: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.
# Epoch 78/100
# 250/250 [==============================] - 6875s 28s/step - loss: 17.2537 - val_loss: 20.2094
# Epoch 79/100
# 250/250 [==============================] - 6899s 28s/step - loss: 16.9336 - val_loss: 19.8521
# Epoch 80/100
# 250/250 [==============================] - 6916s 28s/step - loss: 16.7938 - val_loss: 20.5649
#
# Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
# Epoch 81/100
# 250/250 [==============================] - 6849s 27s/step - loss: 16.8173 - val_loss: 19.9847
# Epoch 82/100
# 250/250 [==============================] - 6887s 28s/step - loss: 16.4461 - val_loss: 20.5967
# Epoch 83/100
# 250/250 [==============================] - 6851s 27s/step - loss: 16.3770 - val_loss: 19.7560
#
# Epoch 00083: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.
# Epoch 84/100
# 250/250 [==============================] - 6869s 27s/step - loss: 16.3934 - val_loss: 19.6639
# Epoch 85/100
# 250/250 [==============================] - 6845s 27s/step - loss: 16.3798 - val_loss: 19.9562
# Epoch 86/100
# 250/250 [==============================] - 6945s 28s/step - loss: 16.3619 - val_loss: 19.6655
# Epoch 87/100
# 250/250 [==============================] - 6911s 28s/step - loss: 16.4665 - val_loss: 20.4367
#
# Epoch 00087: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.
# Epoch 88/100
# 250/250 [==============================] - 6835s 27s/step - loss: 16.5187 - val_loss: 20.1885
# Epoch 89/100
# 250/250 [==============================] - 6882s 28s/step - loss: 16.3341 - val_loss: 20.6531
# Epoch 90/100
# 250/250 [==============================] - 6926s 28s/step - loss: 16.1548 - val_loss: 19.7809
#
# Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.
# Epoch 91/100
# 250/250 [==============================] - 6900s 28s/step - loss: 16.1497 - val_loss: 19.4709
# Epoch 92/100
# 250/250 [==============================] - 6878s 28s/step - loss: 16.1321 - val_loss: 19.7979
# Epoch 93/100
# 250/250 [==============================] - 6930s 28s/step - loss: 16.1987 - val_loss: 20.3343
# Epoch 94/100
# 250/250 [==============================] - 6905s 28s/step - loss: 16.2720 - val_loss: 19.4432
# Epoch 95/100
# 250/250 [==============================] - 6870s 27s/step - loss: 16.2285 - val_loss: 20.1291
# Epoch 96/100
# 250/250 [==============================] - 6861s 27s/step - loss: 16.2629 - val_loss: 19.7109
# Epoch 97/100
# 250/250 [==============================] - 6916s 28s/step - loss: 16.2038 - val_loss: 19.6174
#
# Epoch 00097: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.
# Epoch 98/100
# 250/250 [==============================] - 6919s 28s/step - loss: 15.9782 - val_loss: 20.3714
# Epoch 99/100
# 250/250 [==============================] - 6892s 28s/step - loss: 16.1007 - val_loss: 19.7083
# Epoch 100/100
# 250/250 [==============================] - 6885s 28s/step - loss: 16.2420 - val_loss: 19.8869

